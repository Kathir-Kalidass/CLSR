# System Architecture Overview

## Document Purpose
This document provides a high-level overview of the complete system architecture for the Real-Time Vision-Based Continuous Indian Sign Language Recognition and Translation System. For detailed modular flows, see [03_complete_modular_flow.md](03_complete_modular_flow.md).

---

## Training & Deployment Architecture

### Training Environment: Google Colab
- **GPU Acceleration:** Tesla T4/V100/A100 (free tier available)
- **Dataset Storage:** Google Drive mounted for iSign DB access
- **Training Pipeline:** Full model training with GPU compute
- **Sliding-Window Inference:** Efficient continuous sequence processing (64-frame windows)
- **Checkpointing:** Automatic save to Google Drive
- **Monitoring:** TensorBoard integration for loss/metrics tracking

### Deployment Environment: Local System
- **Webcam Access:** Real-time video capture (cv2.VideoCapture)
- **CPU Inference:** Lightweight inference without GPU requirement
- **Browser Limitation:** Colab cannot access local webcam due to sandbox restrictions
- **Model Loading:** Download trained checkpoint from Colab/Drive
- **Real-Time Processing:** <500ms latency with sliding-window approach

### Why This Split?
1. **Training requires GPU** (hours of compute) â†’ Colab provides free GPU
2. **Deployment requires webcam** (local hardware) â†’ Must run on local machine
3. **Sliding-window enables real-time continuous recognition** without full buffering
4. **Cost-effective:** Free Colab GPU + consumer-grade local CPU

---

## 1. System Architecture Philosophy

The proposed architecture follows **five core design principles**:

### 1.1 Modularity
- Each component solves a specific sub-problem
- Independent development and testing
- Easy replacement of individual modules

### 1.2 Scalability
- Supports multiple datasets (ASL, ISL)
- Configurable vocabulary sizes (100 to 2000+ glosses)
- Adaptable to different sign languages

### 1.3 Real-Time Performance
- Target latency: **<500ms** end-to-end
- Asynchronous processing pipelines
- Optimized inference (quantization, pruning)

### 1.4 Robustness
- Multi-feature learning (RGB + Pose)
- Attention-based adaptive fusion
- Handles variations in lighting, background, signer appearance

### 1.5 Research Alignment
- Based on established base paper ("Multi-Feature Attention Mechanism")
- Extends with state-of-the-art temporal modeling
- Follows transfer learning best practices

---

## 2. Global System Architecture

The architecture is organized into **four major functional layers**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAYER 1: INPUT & PREPROCESSING                â”‚
â”‚  â€¢ Video acquisition (camera/file)                               â”‚
â”‚  â€¢ Frame extraction and normalization                            â”‚
â”‚  â€¢ Pose/landmark extraction                                      â”‚
â”‚  â€¢ Temporal standardization                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LAYER 2: FEATURE EXTRACTION & FUSION                  â”‚
â”‚  â€¢ RGB Stream: CNN-based spatial features                        â”‚
â”‚  â€¢ Pose Stream: Keypoint-based motion features                   â”‚
â”‚  â€¢ Attention-Based Fusion (Base Paper Core)                      â”‚
â”‚  â€¢ Fused multi-modal representation                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LAYER 3: CONTINUOUS SIGN RECOGNITION                    â”‚
â”‚  â€¢ Temporal Modeling (BiLSTM/Transformer)                        â”‚
â”‚  â€¢ CTC Alignment (unsegmented sequences)                         â”‚
â”‚  â€¢ Decoding Strategy (Greedy/Beam Search)                        â”‚
â”‚  â€¢ ISL Gloss Sequence Generation                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LAYER 4: LANGUAGE PROCESSING & OUTPUT                   â”‚
â”‚  â€¢ Caption buffering and merging                                 â”‚
â”‚  â€¢ Seq2Seq translation (ISL gloss â†’ English)                     â”‚
â”‚  â€¢ Grammar correction and refinement                             â”‚
â”‚  â€¢ Text display + Text-to-Speech synthesis                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. End-to-End Data Flow

### 3.1 Input to Output Pipeline

```
ISL Signer â†’ Camera Capture
                â†“
        [RGB Video Stream]
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PREPROCESSING â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Normalized RGB Frames â”‚
    â”‚  + Pose Keypoints      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DUAL-STREAM EXTRACTION     â”‚
    â”‚  â€¢ RGB: CNN (ResNet/I3D)    â”‚
    â”‚  â€¢ Pose: GCN/MLP/RNN        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ATTENTION-BASED FUSION      â”‚
    â”‚  (Base Paper Contribution)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  TEMPORAL MODELING           â”‚
    â”‚  â€¢ BiLSTM or Transformer     â”‚
    â”‚  â€¢ CTC Alignment             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DECODING                    â”‚
    â”‚  â€¢ Greedy / Beam Search      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    [ISL Gloss Sequence]
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LANGUAGE TRANSLATION        â”‚
    â”‚  â€¢ Seq2Seq (Gloss â†’ Text)    â”‚
    â”‚  â€¢ Grammar Correction        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    [English Sentence]
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  TEXT-TO-SPEECH              â”‚
    â”‚  â€¢ TTS Engine                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    Text Display + Audio Output
```

### 3.2 Data Representations at Each Stage

| Stage | Data Representation | Dimensions |
|-------|---------------------|------------|
| **Input** | RGB video frames | (T Ã— H Ã— W Ã— 3) |
| **Preprocessing** | Normalized frames + keypoints | (T Ã— 224 Ã— 224 Ã— 3) + (T Ã— N_kp Ã— D) |
| **RGB Features** | CNN embeddings | (T Ã— D_rgb) |
| **Pose Features** | Encoded keypoints | (T Ã— D_pose) |
| **Fused Features** | Attention-weighted | (T Ã— D_fusion) |
| **Temporal Features** | Contextualized | (T Ã— D_model) |
| **CTC Output** | Gloss probabilities | (T Ã— |Vocab|) |
| **Decoded** | Gloss sequence | List of gloss tokens |
| **Translated** | English text | String |
| **Audio** | Speech waveform | Audio array |

---

## 4. Module Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ video stream
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video Capture &    â”‚â—„â”€â”€â”€ Config: FPS, Resolution
â”‚  Frame Extraction   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚     â”‚
       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                   â”‚
       â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RGB Frame    â”‚    â”‚ Pose Estimation â”‚
â”‚ Normalizationâ”‚    â”‚ (MediaPipe/     â”‚
â”‚              â”‚    â”‚  OpenPose)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â”‚ RGB tensors          â”‚ Keypoint tensors
       â”‚                      â”‚
       â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CNN Backboneâ”‚    â”‚  Pose Encoder   â”‚
â”‚ (ResNet/I3D) â”‚    â”‚  (GCN/MLP/RNN)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â”‚ RGB features         â”‚ Pose features
       â”‚                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Attention Fusion    â”‚â—„â”€â”€â”€ Base Paper Core
       â”‚  â€¢ Temporal Attn     â”‚
       â”‚  â€¢ Cross-modal Attn  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Fused features
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Temporal Encoder    â”‚
       â”‚  â€¢ BiLSTM / Trans.   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Contextualized features
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     CTC Layer        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Frame-level probs
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  CTC Decoder         â”‚â—„â”€â”€â”€ Greedy or Beam Search
       â”‚  (Greedy/Beam)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Gloss sequence
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Caption Buffer      â”‚
       â”‚  & Token Accumulator â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Buffered glosses
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Seq2Seq Translator  â”‚
       â”‚  (Gloss â†’ English)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Raw English text
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Grammar Correction  â”‚
       â”‚  & Refinement        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Corrected text
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                â”‚
                  â–¼                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Text Display â”‚   â”‚  TTS Engine  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Audio
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Speaker    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Core Architectural Components

### 5.1 Base Paper Components (Reused)

#### 5.1.1 Dual-Stream Feature Extraction
**From:** "Deep Learning-Based Sign Language Recognition Using Efficient Multi-Feature Attention Mechanism"

**RGB Stream:**
- Captures appearance, texture, color information
- CNN backbone (ResNet-18/50, I3D, C3D)
- Pretrained on ImageNet/Kinetics
- Output: Spatial-temporal features

**Pose Stream:**
- Captures geometric structure and motion
- Input: 2D/3D keypoints (body, hands, face)
- Encoder: ST-GCN, MLP, or RNN
- Output: Skeletal motion features

**Why Dual-Stream?**
- Complementary information
- RGB: Robust to skeletal tracking errors
- Pose: Robust to appearance variations
- Combined: Higher accuracy (+5-8% from base paper)

#### 5.1.2 Attention-Based Fusion
**From:** Base paper's core innovation

**Mechanism:**
1. **Temporal Self-Attention** (per modality)
   - Identify important time steps
   - Focus on discriminative frames
   - Reduce noise from transition frames

2. **Cross-Modal Attention**
   - Adaptive weighting of RGB vs Pose
   - Context-dependent fusion
   - Formula: `F_fused = Î±(t) Ã— F_rgb(t) + Î²(t) Ã— F_pose(t)`
   - Î±, Î² learned per frame

**Advantages:**
- Outperforms simple concatenation
- Adapts to input quality (e.g., poor lighting â†’ rely more on pose)
- Interpretable (attention weights show modality importance)

### 5.2 Extended Components (Our Contributions)

#### 5.2.1 Temporal Sequence Modeling
**Why Needed:** Base paper handles isolated signs; we need continuous recognition

**Options:**

**A. Bidirectional LSTM (BiLSTM)**
- Forward + backward context
- Proven for sequential data
- Lower computational cost
- Good for real-time systems

**B. Transformer Encoder**
- Multi-head self-attention
- Better long-range dependencies
- Parallel processing (faster training)
- State-of-the-art for sequences

**Implementation:**
- Input: Fused features (T Ã— D_fusion)
- Layers: 2-4 BiLSTM or 6-8 Transformer layers
- Output: Contextualized embeddings (T Ã— D_model)

#### 5.2.2 CTC Alignment
**Why Needed:** Continuous signing has no frame-level annotations

**How CTC Works:**
1. Model outputs probability distribution per frame
2. CTC allows repetitions and blank tokens
3. Decoding collapses to gloss sequence
4. Training: Aligns predictions to ground truth glosses

**Advantages:**
- No need for frame-level labels
- Handles variable signing speeds
- Supports co-articulation

#### 5.2.3 Language Processing Pipeline
**Components:**

1. **Caption Buffering**
   - Accumulate gloss tokens in sliding window
   - Filter duplicates
   - Temporal ordering

2. **Gloss-to-Text Translation**
   - Seq2Seq model (LSTM or Transformer)
   - Input: ISL gloss sequence
   - Output: English sentence
   - Trained on parallel ISL-English corpus

3. **Grammar Correction**
   - Rule-based: Fix common ISLâ†’English patterns
   - LM-based: T5/BART fine-tuned on grammar
   - Fluency enhancement

4. **Text-to-Speech**
   - Offline: gTTS, pyttsx3 (fast)
   - Cloud: Google/AWS/Azure TTS (natural)
   - Neural: Tacotron 2 (best quality)

---

## 6. Key Architectural Decisions

### 6.1 Why Multi-Feature Learning?

**Decision:** Use RGB + Pose dual-stream (from base paper)

**Justification:**
- RGB alone: Sensitive to lighting, clothing, background
- Pose alone: Sensitive to tracking errors, occlusions
- Combined: Robust to both failure modes
- Base paper shows +5-8% accuracy improvement

**Alternative Considered:** RGB-only with data augmentation  
**Why Rejected:** Lower accuracy, less robust to appearance variations

### 6.2 Why Attention-Based Fusion?

**Decision:** Adaptive attention weighting (from base paper)

**Justification:**
- Better than concatenation (+3-5% accuracy)
- Adapts to input quality (e.g., poor lighting)
- Interpretable (attention weights visualizable)

**Alternative Considered:** Simple concatenation + MLP  
**Why Rejected:** Fixed fusion, no adaptability

### 6.3 Why CTC Alignment?

**Decision:** Use CTC for continuous sign recognition

**Justification:**
- No frame-level annotations needed
- Handles variable signing speeds
- Proven for speech recognition (similar problem)

**Alternative Considered:** Sliding window detection  
**Why Rejected:** Requires segmentation, misses co-articulation

### 6.4 Why BiLSTM vs Transformer?

**Decision:** Support both, choose based on use case

**BiLSTM:**
- Pros: Lower latency, less memory, good for real-time
- Cons: Sequential processing, limited long-range

**Transformer:**
- Pros: Better accuracy, parallel training, SOTA
- Cons: Higher latency, more memory

**Recommendation:**
- **Real-time deployment:** BiLSTM
- **Offline/batch processing:** Transformer

### 6.5 Why Language Correction?

**Decision:** Add gloss-to-text translation + grammar correction

**Justification:**
- Raw gloss output is incomprehensible to non-signers
- ISL grammar differs from English (SOV vs SVO)
- Improves usability significantly

**Alternative Considered:** Output glosses only  
**Why Rejected:** Poor user experience

---

## 7. Architecture Advantages

### 7.1 Modularity
- Easy to replace individual components
- Independent testing and debugging
- Supports incremental development

### 7.2 Scalability
- Works with 100 to 2000+ glosses
- Adaptable to other sign languages
- Cloud or edge deployment

### 7.3 Research Alignment
- Based on peer-reviewed base paper
- Extends with state-of-the-art techniques
- Reproducible experiments

### 7.4 Real-Time Performance
- Optimized inference pipelines
- Asynchronous processing
- Target <500ms latency achievable

### 7.5 Robustness
- Multi-feature learning
- Attention-based adaptive fusion
- Handles variations in environment and signer

---

## 8. Architecture Limitations & Mitigations

### 8.1 Limitation: Requires Good Pose Estimation
**Mitigation:**
- Use robust pose estimators (MediaPipe Holistic)
- Fallback to RGB-only if pose fails
- Attention fusion reduces weight on poor pose data

### 8.2 Limitation: Limited ISL Training Data
**Mitigation:**
- Transfer learning from ASL (pretrain on large-scale data)
- Data augmentation (speed, rotation, cropping)
- Fine-tuning strategy (freeze-unfreeze)

### 8.3 Limitation: Real-Time Latency Constraints
**Mitigation:**
- Use efficient backbones (ResNet-18 vs ResNet-50)
- Model quantization (FP16/INT8)
- Asynchronous processing pipelines

### 8.4 Limitation: Grammar Correction Quality
**Mitigation:**
- Use pretrained LMs (T5, BART)
- Fine-tune on ISL-English parallel corpus
- Rule-based post-processing for common errors

---

## 9. Color Coding Convention (For Diagrams)

To clearly communicate architectural contributions:

| Color | Meaning | Examples |
|-------|---------|----------|
| ðŸŸ¢ **Green** | Existing from literature | CNN backbone, Pose estimation |
| ðŸ”µ **Blue** | Modified algorithms | Attention fusion (adapted from base paper) |
| ðŸ”´ **Pink/Red** | Novel contributions | Caption buffering, Language correction |

**Usage:**
- In presentations: Use these colors consistently
- In papers: Explicitly label "from [base paper]" vs "our contribution"
- In code: Document module sources

---

## 10. Related Documents

**For detailed modular flows:** [03_complete_modular_flow.md](03_complete_modular_flow.md)  
**For algorithmic details:** [04_algorithmic_design.md](04_algorithmic_design.md)  
**For model architecture:** [05_model_architecture_details.md](05_model_architecture_details.md)  
**For implementation:** [DETAILED_WORKFLOW.md](DETAILED_WORKFLOW.md)  
**For presentation:** [QUICK_REFERENCE.md](QUICK_REFERENCE.md)

---

**Document Version:** 1.0  
**Last Updated:** January 24, 2026  
**Purpose:** High-level system architecture overview
